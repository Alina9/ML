{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекуррентные сети пишут тексты\n",
    "__Суммарное количество баллов: 12__\n",
    "\n",
    "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
    "\n",
    "__Тема письма: `[ML][MS][HW06] <ФИ>`, где вместо `<ФИ>` указаны фамилия и имя__\n",
    "\n",
    "В этом задании вам предстоит познакомиться с объединением Deep Learning и NLP. Для начала предстоит построить векторное пространство для словоря, а затем применить его для предсказания следующих слов в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = json.load(open(\"opencorp.json\", \"r\", encoding=\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', '«', 'школа', 'злословия', '»', 'учит', 'прикусить', 'язык', 'сохранится', 'ли', 'градус', 'дискуссии', 'в', 'новом', 'сезоне', '?', '<END>']\n",
      "['<START>', 'великолепная', '«', 'школа', 'злословия', '»', 'вернулась', 'в', 'эфир', 'после', 'летних', 'каникул', 'в', 'новом', 'формате', '.', 'в', 'истории', 'программы', 'это', 'уже', 'не', 'первый', '«', 'ребрендинг', '»', '.', 'сейчас', 'с', 'трудом', 'можно', 'припомнить', ',', 'что', 'начиналась', '«', 'школа', '…', '»', 'на', 'канале', '«', 'культура', '»', 'как', 'стандартное', 'ток-шоу', ',', 'которое', 'отличалось', 'от', 'других', '«', 'кухонными', '»', 'обсуждениями', 'гостя', ',', 'что', 'называется', '–', '«', 'за', 'глаза', '»', ',', 'и', 'неожиданными', 'персонами', 'в', 'качестве', 'ведущих', '.', '<END>']\n",
      "['<START>', 'писательница', 'татьяна', 'толстая', 'и', 'сценаристка', 'дуня', 'смирнова', 'вроде', 'бы', 'не', 'вполне', 'соответствовали', 'принятым', 'на', 'российском', 'телевидении', 'стандартам', 'телеведущих', '.', '<END>']\n"
     ]
    }
   ],
   "source": [
    "for sentence in tokenized[:3]:\n",
    "    sentence =  ['<START>'] + sentence + ['<END>']\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 (3 балла)\n",
    "Чтобы обучить нейронную сеть, нам нужен датасет. В данном задании предлагается использовать данные, полученные из корпуса текстов OpenCorpora. Более того, датасет нужно представить в удобном виде. Поскольку мы хотим обучать эмбеддинги на парах `(token_center, token_context)`, а также иметь возможность делать `negative sampling`, датасет должен уметь выдавать соответствующие пары, а так же `negative sampling`-токены. \n",
    "\n",
    "Кроме того, мы бы не хотели строить эмбеддинги для очень редких слов, поэтому в словарь и в пары должны входить только слова, которые встречаются более `count_threshold` раз, а остальные должны быть заменены на специальный токен `\"<UNKNOWN>\"`. Последовательность должна начинаться с токена `\"<START>\"` и заканчиваться токеном `\"<END>\"`.\n",
    "\n",
    "#### Методы\n",
    "`__init__` - принимает на вход список последовтельностей токенов, преобразуя в соответствии с описанными выше критериями. При инициализации списка токенов нужно учитывать, что с вероятностью $1 - (\\sqrt{\\frac{0.001}{f(t)}} + 1) \\cdot \\frac{f(t)}{0.001}$ ($f(t)$ - частота слова в корпусе) мы \"выкидываем\" слово из текста, не добавляя никакие пары токенов с его участием в список. Это нужно для того, чтобы мы не переобучались на часто встречаемые слова. Также в `self.voc` должен записать актуальный словарь токенов.\n",
    "\n",
    "`__len__` - возвращает количество пар `(token_center, token_context)`\n",
    "\n",
    "`__getitem__` - принимает на вход индекс `i`, соответствующий паре `(t_i, c_i)`. Возвращает пару тензоров `(t_i, [c_i] + negatives)`, где `negatives` - список негативных токенов.\n",
    "\n",
    "`negative_sampling` - осуществляет взвешенное негативное сэмплирование. Вес токена определяется как $\\frac{(count(t))^{0.75}}{\\sum (count(t))^{0.75}}$, т.е. в negative samples частые слова попадают чаще, чем другие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, tokenized_sources, window=3, count_threshold=5, negative_sampling=5):\n",
    "        self.token_pairs = []\n",
    "        self.window = window\n",
    "        self.count_threshold = count_threshold\n",
    "        self.neg_sampling = negative_sampling\n",
    "        self.voc, self.counts = self.make_dictionary(tokenized_sources)\n",
    "        self.size = len(self.voc)\n",
    "        \n",
    "        #self.voc = set([\"<START>\", \"<END>\", \"<UNKNOWN>\"])\n",
    "        #self.counts = {\"<START>\": len(tokenized_sources), \"<END>\": len(tokenized_sources), \"<UNKNOWN>\":1}\n",
    "        \n",
    "        self.idx2token = list(self.voc)\n",
    "        self.token2idx = {token: i for i, token in enumerate(self.idx2token)}\n",
    "        \n",
    "        self.pair = self.make_pair(tokenized_sources)\n",
    "        sum_count = sum(np.array(list(self.counts.values()))**0.75)\n",
    "        self.negative_prob = [v**0.75/sum_count for v in self.counts.values()]\n",
    "                    \n",
    "    def negative_sampling(self):\n",
    "        return np.random.choice(np.array(self.size), size=self.neg_sampling, replace=False, p=self.negative_prob).tolist()\n",
    "    \n",
    "    def make_dictionary(self, tokenized):\n",
    "        dictionary = set()\n",
    "        tokens = [token for sentence in tokenized for token in sentence]\n",
    "        \n",
    "        counter = dict(Counter(tokens))\n",
    "        unknown = []\n",
    "        for token, n in counter.items():\n",
    "            if n > self.count_threshold:\n",
    "                dictionary.add(token)\n",
    "            else:\n",
    "                unknown.append(token)\n",
    "        for token in unknown:\n",
    "            del counter[token]\n",
    "\n",
    "                \n",
    "        counter[\"<UNKNOWN>\"] = len(unknown)\n",
    "        counter[\"<START>\"] = len(tokenized)\n",
    "        counter[\"<END>\"] = len(tokenized)\n",
    "        dictionary = dictionary | set([\"<START>\", \"<END>\", \"<UNKNOWN>\"])\n",
    "        return dictionary, counter\n",
    "    \n",
    "    def make_pair(self, tokenized):\n",
    "        pair = []\n",
    "        num = self.size\n",
    "        \n",
    "        probs = {item[0]: (np.sqrt((item[1]/num)/0.001) + 1)*(0.001/(item[1]/num)) for item in self.counts.items()}## вероятность оставить слово\n",
    "        \n",
    "        for sentence in tokenized:\n",
    "            sentence =  ['<START>'] + sentence + ['<END>']\n",
    "            new_sentence = [] ## записываю индекс токена в словаре\n",
    "            \n",
    "            for center in sentence:\n",
    "                if center in self.voc: \n",
    "                    if probs[center] < np.random.uniform(0, 1) or center =='<START>' or center == '<END>':\n",
    "                        new_sentence.append(self.token2idx[center])\n",
    "\n",
    "            \n",
    "            for center in range(len(new_sentence)):\n",
    "                \n",
    "                l = max(0, center - self.window)\n",
    "                r = min(len(new_sentence) - 1, center + self.window)\n",
    "                \n",
    "                for i in range(l, center):\n",
    "                    pair.append((new_sentence[center],new_sentence[i]))\n",
    "                \n",
    "                for i in range(center + 1, r):\n",
    "                    pair.append((new_sentence[center],new_sentence[i]))\n",
    "                    \n",
    "        return pair                      \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        elem1, elem2 = self.pair[index]\n",
    "        return torch.tensor([elem1], dtype=torch.long), torch.tensor([elem2] + self.negative_sampling(), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_pairs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TokenDataset(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 (4 балла)\n",
    "Теперь реализуем непосредственно SkipGram. Для этого нам потребуются `torch.autograd.Variables` чтобы сделать эмбеддинги обучаемыми. Также сразу реализуем интерфейс, которым будем пользоваться для применения эмбеддингов в следующих задачах.\n",
    "\n",
    "#### Методы SkipGram\n",
    "`__init__` - принимает на вход словарь и размерность пространств эмбеддингов. Инициализирует эмбеддинги для центрального и контекстного слов.\n",
    "\n",
    "`get_variables` - возвращает лист из всех `torch.autograd.Variables`. Необходимо, чтобы инициализировать оптимизатор.\n",
    "\n",
    "`predict_proba(center_tokens, context_tokens)` - принимает на вход список центральных токенов и список списков токенов из предполагаемого контекста. Для каждого центрального токена и соответствующего ему списка контекстных токенов должен вернуть скалярное произведение центрального и контекстуального эмбеддингов.\n",
    "\n",
    "#### Методы Embedding\n",
    "`__init__` - принимает на вход обученный SkipGram\n",
    "\n",
    "`embed` - возвращает эмбеддинги для всех элементов списка\n",
    "\n",
    "`reconstruct` - для всех элементов списка возвращает наиболее подходящий токен. Не возвращает `\"<UNKNOWN>\"`\n",
    "\n",
    "`n_closest` - возвращает `n` ближайших токенов для каждого элемента списка. Не возвращает `\"<UNKNOWN>\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram:\n",
    "    def __init__(self, voc, latent_size):\n",
    "        self.id2token = list(voc)\n",
    "        self.token2id = dict([(token, i) for i, token in enumerate(self.id2token)])\n",
    "        self.latent_size = latent_size\n",
    "        self.W_center = torch.randn(len(voc), self.latent_size, requires_grad=True)\n",
    "        self.W_context = torch.randn(self.latent_size, len(voc), requires_grad=True)\n",
    "        \n",
    "    def get_variables(self):\n",
    "        return [self.W_center, self.W_context]\n",
    "    \n",
    "    def predict_proba(self, center_ids, context_ids):\n",
    "        center = self.W_center[center_ids]\n",
    "        context = self.W_context[:, context_ids].transpose(1, 0)\n",
    "        return torch.bmm(center, context).reshape(context_ids.size())\n",
    "    \n",
    "class Embedding:\n",
    "    def __init__(self, skip_gram):\n",
    "        self.skip_gram = skip_gram\n",
    "        self.W_center = skip_gram.W_center.detach().numpy()\n",
    "        self.W_context = skip_gram.W_context.transpose(1, 0).detach().numpy()\n",
    "        self.tree = KDTree(matrix, leaf_size=1, metric=\"euclidean\")\n",
    "    \n",
    "    def embed(self, tokens):\n",
    "        ind = self.skip_gram.token2id[tokens]\n",
    "        return self.W_center[ind] + self.W_context[ind]\n",
    "    \n",
    "    def reconstruct(self, embeddings):\n",
    "        return n_closest(self, embeddings, n=1)\n",
    "\n",
    "    def n_closest(self, embeddings, n=5):\n",
    "        neighbors = self.tree.quary(embeddings, k = n+1, return_distance=False)[0][1:]\n",
    "        closest = []\n",
    "        i=0\n",
    "        while i<n:\n",
    "            neighbor = self.skip_gram.id2token[neighbors[i]]\n",
    "            if neighbor != '<UNKNOWN>':\n",
    "                closest.append(neighbor)\n",
    "            i+=1\n",
    "        return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-1f9889425cb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating dataset...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_sampling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Creating skipgram...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mskipgram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSkipGram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context)\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# map-style\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_source, replacement, num_samples)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[1;32m---> 94\u001b[1;33m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4096\n",
    "EPOCHS = 200\n",
    "\n",
    "print(\"Creating dataset...\")\n",
    "dataset = TokenDataset(tokenized, count_threshold=10, window=4, negative_sampling=10)\n",
    "dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "print(\"Creating skipgram...\")\n",
    "skipgram = SkipGram(dataset.voc, 150)\n",
    "optim = torch.optim.Adam(skipgram.get_variables(), lr=1e-3)\n",
    "y = torch.tensor([0] * (BATCH_SIZE + 1), dtype=torch.long).cuda()\n",
    "losses = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    avg_loss = 0\n",
    "    steps = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        probs = skipgram.predict_proba(batch[0], batch[1])\n",
    "        loss = F.cross_entropy(probs, y[:len(probs)])\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        avg_loss += loss.item()\n",
    "        steps += 1\n",
    "    losses.append(avg_loss/steps)\n",
    "    print(\"Loss:\", avg_loss/steps)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(list(range(EPOCHS)), losses)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedding(skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king = embedder.embed([\"король\"])[0]\n",
    "cat = embedder.embed([\"кошка\"])[0]\n",
    "owl = embedder.embed([\"сыч\"])[0]\n",
    "give = embedder.embed([\"дать\"])[0]\n",
    "me = embedder.embed([\"ты\"])[0]\n",
    "you = embedder.embed([\"я\"])[0]\n",
    "print(embedder.n_closest([king], 10)[0])\n",
    "print(embedder.n_closest([cat], 10)[0])\n",
    "print(embedder.n_closest([owl], 10)[0])\n",
    "print(embedder.n_closest([give], 10)[0])\n",
    "print(embedder.n_closest([you], 10)[0])\n",
    "print(embedder.n_closest([me], 10)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 (2 балла)\n",
    "Теперь будем учиться восстанавливать слова в тексте. Для этого нам потребуется также определить датасет последовательностей фиксированной длинны.\n",
    "\n",
    "#### Методы\n",
    "`__init__` - принимает на вход `embedder` (обученный SkipGram) и список токенизированных последоватлеьностей `tokenized`.\n",
    "\n",
    "`__getitem__` - возвращает случайную закодированную при помощи SkipGram подпоследовательность длины `seq_len` одной из исходных последовательностей, сдвинутую на один токен подпоследовательность (т.е. следующие слова в тексте) и маску, которая отражает то, является ли токен неизвестным (`\"<UNKNOWN\"`).\n",
    "\n",
    "`__len__` - равна количеству последовательностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenSeqDataset(Dataset):\n",
    "    def __init__(self, embedder, tokenized, seq_len=32):\n",
    "        self.seq_len = seq_len\n",
    "            \n",
    "    def __len__(self):\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4 (2 балла)\n",
    "Теперь обучим рекуррентную сеть, которая будет предсказывать следующее слово в тексте. Модель будет состоять из трех блоков: `input` (отвечает за предоброботку эмбеддинга), `rnn` (рекуррентная часть), `output` (отвечает за постобработку выхода).\n",
    "\n",
    "#### Методы\n",
    "`predict_sequential` - возвращает последовательность предсказаний для батча последовательностей\n",
    "\n",
    "`get_next` - предсказывает следующее слово\n",
    "\n",
    "`reset` - обнуляет внутреннее состояние сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN:\n",
    "    def __init__(self, latent_space=150, hidden_layer=512):\n",
    "        self.input = torch.nn.Sequential(\n",
    "            torch.nn.Linear(latent_space, hidden_layer),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_layer, hidden_layer),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.rnn = None\n",
    "        self.output = None\n",
    "        self.input.cuda()\n",
    "        self.rnn.cuda()\n",
    "        self.output.cuda()\n",
    "        self.hidden = None\n",
    "    \n",
    "    def predict_sequential(self, sequences):\n",
    "        x, _ = self.rnn(self.input(sequences))\n",
    "        return self.output(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return list(self.input.parameters()) + list(self.rnn.parameters()) + list(self.output.parameters())\n",
    "    \n",
    "    def get_next(self, batch):\n",
    "        x, self.hidden = self.rnn(self.input(sequences).unsqueeze(1))\n",
    "        return self.output(x)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.hidden = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 200\n",
    "\n",
    "dataset = TokenSeqDataset(embedder, tokenized)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "rnn = TextRNN(150)\n",
    "optim = torch.optim.Adam(rnn.parameters(), lr=1e-3)\n",
    "losses = []\n",
    "top1accs = []\n",
    "top5accs = []\n",
    "for i in range(EPOCHS):\n",
    "    avg_loss = 0\n",
    "    top1acc = 0\n",
    "    top5acc = 0\n",
    "    steps = 0\n",
    "    acc_steps = 0\n",
    "    for x, y_true, loss_mask in tqdm(dataloader):\n",
    "        y_pred = rnn.predict_sequential(x.cuda())\n",
    "        loss = (((y_true.cuda() - y_pred)**2).mean(dim=-1) * loss_mask.cuda()).mean()\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        avg_loss += loss.item()\n",
    "        steps += 1\n",
    "        if steps % 100 == 0:\n",
    "            acc_steps += 1\n",
    "            word_pred = embedder.n_closest(y_pred.detach().view(-1, 150)[:200].cpu().numpy())\n",
    "            word_true = embedder.reconstruct(y_true.detach().view(-1, 150)[:200].cpu().numpy())\n",
    "            unknown_mask = loss_mask.view(-1).cpu().numpy()\n",
    "            t1a = 0\n",
    "            t5a = 0\n",
    "            for true, pred, is_unknown in zip(word_true, word_pred, loss_mask):\n",
    "                if is_unknown:\n",
    "                    continue\n",
    "                if true == pred[0]:\n",
    "                    t1a += 1\n",
    "                if true in pred:\n",
    "                    t5a += 1\n",
    "            top1acc += t1a / len(word_pred)\n",
    "            top5acc += t5a / len(word_pred)\n",
    "    losses.append(avg_loss/steps)\n",
    "    top1accs.append(top1acc/acc_steps)\n",
    "    top5accs.append(top5acc/acc_steps)\n",
    "    print(\"Loss:\", avg_loss/steps)\n",
    "    print(\"Top-1 accuracy:\", top1acc/acc_steps)\n",
    "    print(\"Top-5 accuracy:\", top5acc/acc_steps)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(list(range(EPOCHS)), losses)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(list(range(100)), top1accs, label=\"Top-1\")\n",
    "plt.plot(list(range(100)), top5accs, label=\"Top-5\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5 (1 балл)\n",
    "Отлично, осталось только научитсья итеративно продолжать последовательность. Давайте попробуем научиться это делать.\n",
    "\n",
    "#### Методы\n",
    "`continue_sequence` - возвращает завершенную последовательность. Входная последовательность может быть пустой, поэтому в начало нужно добавить токен `\"<START>\"`. Закончить построение последовательности нужно после получения токена `\"<END>\"` или после получения `max_len` новых слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceCompleter:\n",
    "    def __init__(self, rnn, embedder, max_len=128):\n",
    "        self.rnn = rnn\n",
    "        self.embedder = embeder\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def continue_sequence(self, sequence):\n",
    "        t_sequence = [\"<START>\"] + sequence\n",
    "        embedding = self.embedder.embed(t_sequence)\n",
    "        self.rnn.reset()\n",
    "        with torch.no_grad():\n",
    "            for e in embedding:\n",
    "                x = self.rnn.get_next(torch.tensor([e], dtype=torch.float).cuda())\n",
    "            rec = self.embeder.reconstruct(x)\n",
    "            continued_sequence = []\n",
    "            ctn = 0\n",
    "            while rec[0] != \"<END>\" and ctn < self.max_len:\n",
    "                continued_sequence.append(rec[0])\n",
    "                e = self.embedder.embed(rec)\n",
    "                x = self.rnn.get_next(torch.tensor(e, dtype=torch.float).cuda())\n",
    "                rec = self.embeder.reconstruct(x)\n",
    "        return sequence + continued_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_completer = SequenceCompleter(rnn, embedder)\n",
    "print(seq_completer.continue_sequence([\"учеба\", \"в\", \"магистратуре\", \"-\", \"это\"]))\n",
    "print(seq_completer.continue_sequence([\"работает\", \"ли\", \"наша\", \"простая\", \"модель\", \"?\"]))\n",
    "print(seq_completer.continue_sequence([\"я\", \"точно\", \"знаю\"]))\n",
    "print(seq_completer.continue_sequence([\"машина\", \"времени\"]))\n",
    "print(seq_completer.continue_sequence([\"сегодня\"]))\n",
    "print(seq_completer.continue_sequence([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM:\n",
    "    def __init__(self, symbols):\n",
    "        self.hidden = None\n",
    "        self.input = torch.nn.Linear(len(symbols), 128)\n",
    "        self.lstm = torch.nn.LSTM(128, 128)\n",
    "        self.output = torch.nn.Sequential(torch.nn.Linear(128, 128), torch.nn.ReLU(), torch.nn.Linear(128, len(symbols)))\n",
    "    \n",
    "    def loss(self, batch):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
