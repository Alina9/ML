{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайные леса\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
    "\n",
    "__Тема письма: `[ML][HW04] <ФИ>`, где вместо `<ФИ>` указаны фамилия и имя__\n",
    "\n",
    "В этом задании вам предстоит реализовать ансамбль деревьев решений, известный как случайный лес, применить его к публичным данным пользователей социальной сети Вконтакте, и сравнить его эффективность с ансамблем, предоставляемым библиотекой CatBoost.\n",
    "\n",
    "В результате мы сможем определить, какие подписки пользователей больше всего влияют на определение возраста и пола человека. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import copy\n",
    "from collections import Counter\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(x):\n",
    "    _, counts = np.unique(x, return_counts=True)\n",
    "    proba = counts / len(x)\n",
    "    return np.sum(proba * (1 - proba))\n",
    "    \n",
    "def entropy(x):\n",
    "    _, counts = np.unique(x, return_counts=True)\n",
    "    proba = counts / len(x)\n",
    "    return -np.sum(proba * np.log2(proba))\n",
    "\n",
    "def gain(left_y, right_y, criterion):\n",
    "    y = np.concatenate((left_y, right_y))\n",
    "    return criterion(y) - (criterion(left_y) * len(left_y) + criterion(right_y) * len(right_y)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 (2 балла)\n",
    "Random Forest состоит из деревьев решений. Каждое такое дерево строится на одной из выборок, полученных при помощи bagging. Элементы, которые не вошли в новую обучающую выборку, образуют out-of-bag выборку. Кроме того, в каждом узле дерева мы случайным образом выбираем набор из `max_features` и ищем признак для предиката разбиения только в этом наборе.\n",
    "\n",
    "Сегодня мы будем работать только с бинарными признаками, поэтому нет необходимости выбирать значение признака для разбиения.\n",
    "\n",
    "#### Методы\n",
    "`predict(X)` - возвращает предсказанные метки для элементов выборки `X`\n",
    "\n",
    "#### Параметры конструктора\n",
    "`X, y` - обучающая выборка и соответствующие ей метки классов. Из нее нужно получить выборку для построения дерева при помощи bagging. Out-of-bag выборку нужно запомнить, она понадобится потом.\n",
    "\n",
    "`criterion=\"gini\"` - задает критерий, который будет использоваться при построении дерева. Возможные значения: `\"gini\"`, `\"entropy\"`.\n",
    "\n",
    "`max_depth=None` - ограничение глубины дерева. Если `None` - глубина не ограничена\n",
    "\n",
    "`min_samples_leaf=1` - минимальное количество элементов в каждом листе дерева.\n",
    "\n",
    "`max_features=\"auto\"` - количество признаков, которые могут использоваться в узле. Если `\"auto\"` - равно `sqrt(X.shape[1])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeLeaf:\n",
    "    def __init__(self, y):\n",
    "        counter = Counter(y)\n",
    "        most_common, _ = counter.most_common(1)[0]\n",
    "        self.y = most_common\n",
    "        total = len(y)\n",
    "        self.probability = {label: (p / total) for label, p in counter.items()}\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        return self.probability\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return self.y\n",
    "    \n",
    "class Node:\n",
    "    def __init__(self, split_dim, left = None, right = None):\n",
    "        self.split_dim = split_dim\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        value = X[self.split_dim]\n",
    "        if value == 0:\n",
    "            return self.left.predict_proba(X)\n",
    "        return self.right.predict_proba(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if X[self.split_dim] == 0:\n",
    "            return self.left.predict(X)        \n",
    "        return self.right.predict(X)\n",
    "    \n",
    "        \n",
    "class DecisionTree:\n",
    "    def __init__(self, X, y, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\"):\n",
    "        self.criterion = criterion \n",
    "        self.n_X, self.n_features = X.shape\n",
    "        if max_depth:\n",
    "            self.max_depth = max_depth\n",
    "        else:\n",
    "            self.max_depth = float(\"inf\")\n",
    "            \n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        \n",
    "        if max_features == \"auto\":\n",
    "            self.max_features = int(np.sqrt(self.n_features))\n",
    "        else:\n",
    "            self.max_features = max_features\n",
    "        \n",
    "        bagging_ind = np.random.choice(self.n_X, size  = self.n_X, replace=True)\n",
    "        self.X = X[bagging_ind]\n",
    "        self.y = y[bagging_ind]\n",
    "        \n",
    "        out_of_bag_ind = [x for x in range(self.n_X) if x not in bagging_ind]\n",
    "        self.X_out  = X[out_of_bag_ind]\n",
    "        self.y_out = y[out_of_bag_ind]\n",
    "        \n",
    "        self.root = self.build(np.arange(self.n_X))\n",
    "        \n",
    "    def build(self, indexes, depth = 0):\n",
    "        if depth >= self.max_depth or len(indexes) <= self.min_samples_leaf:\n",
    "            \n",
    "            return DecisionTreeLeaf(self.y[indexes])\n",
    "\n",
    "        split_gain, split_dim, indexes_left, indexes_right = self.best_split(indexes)\n",
    "\n",
    "\n",
    "        if split_gain == 0:\n",
    "            return DecisionTreeLeaf(self.y[indexes])\n",
    "\n",
    "        left = self.build(indexes_left, depth + 1)\n",
    "        right = self.build(indexes_right, depth + 1)\n",
    "\n",
    "        return Node(split_dim, left, right)\n",
    "\n",
    "    \n",
    "    def best_split(self, indexes):\n",
    "        best_gain = 0\n",
    "        best_dim = 0\n",
    "        min_leaf = self.min_samples_leaf\n",
    "        best_indexes_left = []\n",
    "        best_indexes_right = []\n",
    "        \n",
    "        if self.criterion == \"gini\":\n",
    "            criterion = gini\n",
    "        else:\n",
    "            criterion = entropy\n",
    "        \n",
    "        \n",
    "        self.random_features = np.random.choice(self.n_features, size=self.max_features, replace=False)\n",
    "        \n",
    "        for dim in self.random_features:\n",
    "            X = self.X[indexes,dim]\n",
    "                       \n",
    "            indexes_left = indexes[X == 0]\n",
    "            indexes_right = indexes[X == 1]                     \n",
    "                \n",
    "            if len(indexes_left) >= min_leaf and len(indexes_right) >= min_leaf:\n",
    "            \n",
    "                y = self.y\n",
    "                split_gain = gain(y[indexes_left], y[indexes_right], criterion)\n",
    "                if split_gain > best_gain:\n",
    "                    best_gain = split_gain\n",
    "                    best_dim = dim\n",
    "                    best_indexes_left = indexes_left\n",
    "                    best_indexes_right = indexes_right\n",
    "\n",
    "        return best_gain, best_dim, best_indexes_left, best_indexes_right\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        proba = []\n",
    "        for x in X:\n",
    "            p = self.root.predict_proba(x)\n",
    "            proba.append(p)\n",
    "        return proba            \n",
    "        \n",
    "    def predict(self, X):\n",
    "        pred = []\n",
    "        for x in X:\n",
    "            p = self.root.predict(x)\n",
    "            pred.append(p)    \n",
    "        return pred\n",
    "    \n",
    "    def feature_importance(self):\n",
    "        importance = np.zeros(self.n_features)\n",
    "        err_oob = 1 - np.mean(self.predict(self.X_out) == self.y_out)\n",
    "        \n",
    "        for i in range(self.n_features):\n",
    "            X_shuffle = self.X_out.copy()\n",
    "            np.random.shuffle(X_shuffle[:, i])\n",
    "            err_oob_j = 1 - np.mean(self.predict(X_shuffle) == self.y_out)\n",
    "            importance[i] = err_oob_j - err_oob\n",
    "\n",
    "        return importance\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 (2 балла)\n",
    "Теперь реализуем сам Random Forest. Идея очень простая: строим `n` деревьев, а затем берем модальное предсказание.\n",
    "\n",
    "#### Параметры конструктора\n",
    "`n_estimators` - количество используемых для предсказания деревьев.\n",
    "\n",
    "Остальное - параметры деревьев.\n",
    "\n",
    "#### Методы\n",
    "`fit(X, y)` - строит `n_estimators` деревьев по выборке `X`.\n",
    "\n",
    "`predict(X)` - для каждого элемента выборки `X` возвращает самый частый класс, который предсказывают для него деревья."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(self, criterion=\"gini\", max_depth=None, min_samples_leaf=1, max_features=\"auto\", n_estimators=10):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimators = []\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        i = 0\n",
    "        self.y_type = y.dtype\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_X = X.shape[0]\n",
    "        while i < self.n_estimators:\n",
    "            e = DecisionTree(X, y, self.criterion, self.max_depth, self.min_samples_leaf, self.max_features)\n",
    "            i+=1\n",
    "            self.estimators.append(e)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = []\n",
    "        res = np.empty((X.shape[0], self.n_estimators), dtype = self.y_type)\n",
    "        i = 0\n",
    "        for e in self.estimators: \n",
    "            res[:,i] = e.predict(X)\n",
    "            i+=1\n",
    "            \n",
    "        for r in res:            \n",
    "            p, _ = Counter(r).most_common(1)[0]\n",
    "            pred.append(p)\n",
    "        return pred\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3 (2 балла)\n",
    "Часто хочется понимать, насколько большую роль играет тот или иной признак для предсказания класса объекта. Есть различные способы посчитать его важность. Один из простых способов сделать это для Random Forest - посчитать out-of-bag ошибку предсказания `err_oob`, а затем перемешать значения признака `j` и посчитать ее (`err_oob_j`) еще раз. Оценкой важности признака `j` для одного дерева будет разность `err_oob_j - err_oob`, важность для всего леса считается как среднее значение важности по деревьям.\n",
    "\n",
    "Реализуйте функцию `feature_importance`, которая принимает на вход Random Forest и возвращает массив, в котором содержится важность для каждого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(rfc):\n",
    "    importance = np.zeros((rfc.n_features, rfc.n_estimators))\n",
    "    i = 0\n",
    "    for e in rfc.estimators:\n",
    "        importance[:,i] = e.feature_importance()\n",
    "        i+=1\n",
    "    return importance.mean(axis = 1)\n",
    "    \n",
    "    \n",
    "def most_important_features(importance, names, k=20):\n",
    "    # Выводит названия k самых важных признаков\n",
    "    idicies = np.argsort(importance)[::-1]\n",
    "    return np.array(names)[idicies[:k]], importance[idicies[:k]], idicies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, пришло время протестировать наше дерево на простом синтетическом наборе данных. В результате должна получиться точность `1.0`, наибольшее значение важности должно быть у признака с индексом `4`, признаки с индексами `2` и `3`  должны быть одинаково важны, а остальные признаки - не важны совсем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Importance: [ 1.00647889e-04 -1.96372863e-03  1.66492368e-01  1.60051188e-01\n",
      "  3.17101966e-01 -1.01424102e-03]\n"
     ]
    }
   ],
   "source": [
    "def synthetic_dataset(size):\n",
    "    X = [(np.random.randint(0, 2), np.random.randint(0, 2), i % 6 == 3, \n",
    "          i % 6 == 0, i % 3 == 2, np.random.randint(0, 2)) for i in range(size)]\n",
    "    y = [i % 3 for i in range(size)]\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = synthetic_dataset(1000)\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "rfc.fit(X, y)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X) == y))\n",
    "print(\"Importance:\", feature_importance(rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4 (1 балл)\n",
    "Теперь поработаем с реальными данными.\n",
    "\n",
    "Выборка состоит из публичных анонимизированных данных пользователей социальной сети Вконтакте. Первые два столбца отражают возрастную группу (`zoomer`, `doomer` и `boomer`) и пол (`female`, `male`). Все остальные столбцы являются бинарными признаками, каждый из них определяет, подписан ли пользователь на определенную группу/публичную страницу или нет.\\\n",
    "\\\n",
    "Необходимо обучить два классификатора, один из которых определяет возрастную группу, а второй - пол.\\\n",
    "\\\n",
    "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются. Лес должен строиться за какое-то разумное время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    dataframe = pandas.read_csv(path, header=0)\n",
    "    dataset = dataframe.values.tolist()\n",
    "    random.shuffle(dataset)\n",
    "    y_age = [row[0] for row in dataset]\n",
    "    y_sex = [row[1] for row in dataset]\n",
    "    X = [row[2:] for row in dataset]\n",
    "    \n",
    "    return np.array(X), np.array(y_age), np.array(y_sex), list(dataframe.columns)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
    "X_train, X_t, y_age_train, y_age_t, y_sex_train, y_sex_t = train_test_split(X, y_age, y_sex, train_size=0.8)\n",
    "X_val, X_test, y_age_val, y_age_test, y_sex_val, y_sex_test = train_test_split(X_t, y_age_t, y_sex_t, train_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.952013164889486"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = np.zeros((100000, 6))\n",
    "# i = 0\n",
    "# #1 - gini, 2 - entropy\n",
    "# for crit in [1, 2]:\n",
    "#     for d in [4, 5, 6]:\n",
    "#         for msl in [5, 10, 30]:\n",
    "#             for mf in [3, 6, 12]:\n",
    "#                 for n_est in [10]:\n",
    "#                     rfc = RandomForestClassifier(criterion=crit, max_depth=d, min_samples_leaf=msl,\n",
    "#                                                        max_features=mf, n_estimators=n_est)\n",
    "#                     rfc.fit(X_train, y_age_train)\n",
    "#                     acc =  np.mean(rfc.predict(X_val) == y_age_val)\n",
    "#                     res[i,:] =[crit, d , msl, mf, n_est , acc]\n",
    "#                     i+=1\n",
    "#                     print(f'criterion = {crit}, ',f'depth = {d},', f'min_samples_leaf = {msl},', f'max_features = {mf}, ',\n",
    "#                          f'n_estimators = {n_est}')\n",
    "#                     print(\"Accuracy:\", np.mean(rfc.predict(X_val) == y_age_val))\n",
    "\n",
    "# print(res[np.argmax(res[:,-1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_res = res[np.argsort(res[:,-1])][::-1]\n",
    "# sort_res[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5863808322824716\n",
      "Most important features:\n",
      "1. styd.pozor 0.022962074479964404\n",
      "2. 4ch 0.019798865311447356\n",
      "3. dayvinchik 0.015666234458350744\n",
      "4. ovsyanochan 0.015522930863289663\n",
      "5. mudakoff 0.01482851597965982\n",
      "6. rhymes 0.012802302419335125\n",
      "7. pixel_stickers 0.006906646416633159\n",
      "8. rapnewrap 0.006548263611826529\n",
      "9. pozor 0.005930884118244524\n",
      "10. tumblr_vacuum 0.005002558216380825\n",
      "11. webestano 0.004991741867459121\n",
      "12. memeboizz 0.004163205039195975\n",
      "13. reflexia_our_feelings 0.004055947744669686\n",
      "14. pravdashowtop 0.0037161070851349566\n",
      "15. iwantyou 0.0030980743266318576\n",
      "16. i_d_t 0.0027934147997337364\n",
      "17. bog_memes 0.002772231921835781\n",
      "18. ne1party 0.0022647906787495544\n",
      "19. leprum 0.0022185511370564717\n",
      "20. ne.poverish 0.0021339794735026675\n"
     ]
    }
   ],
   "source": [
    "rfc = rfc = RandomForestClassifier(criterion='gini', max_depth=6, min_samples_leaf=10,\n",
    "                                                       max_features=\"auto\", n_estimators=30)\n",
    "rfc.fit(X_train, y_age_train)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_age_test))\n",
    "print(\"Most important features:\")\n",
    "importance = feature_importance(rfc)\n",
    "k = 20\n",
    "names, imp, ind = most_important_features(importance, features, k)\n",
    "for i in range(k):\n",
    "    print(str(i+1) + \".\", names[i], imp[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. styd.pozor 0.022962074479964404\n",
      "2. 4ch 0.019798865311447356\n",
      "3. dayvinchik 0.015666234458350744\n",
      "4. ovsyanochan 0.015522930863289663\n",
      "5. mudakoff 0.01482851597965982\n",
      "6. rhymes 0.012802302419335125\n",
      "7. pixel_stickers 0.006906646416633159\n",
      "8. rapnewrap 0.006548263611826529\n",
      "9. pozor 0.005930884118244524\n",
      "10. tumblr_vacuum 0.005002558216380825\n",
      "11. webestano 0.004991741867459121\n",
      "12. memeboizz 0.004163205039195975\n",
      "13. reflexia_our_feelings 0.004055947744669686\n",
      "14. pravdashowtop 0.0037161070851349566\n",
      "15. iwantyou 0.0030980743266318576\n",
      "16. i_d_t 0.0027934147997337364\n",
      "17. bog_memes 0.002772231921835781\n",
      "18. ne1party 0.0022647906787495544\n",
      "19. leprum 0.0022185511370564717\n",
      "20. ne.poverish 0.0021339794735026675\n",
      "21. cook_bon 0.0021071137652326375\n",
      "22. tysnm 0.0020145753905451294\n",
      "23. pustota_diary 0.0018414337972708194\n",
      "24. webmland 0.0017389322016542826\n",
      "25. in.humour 0.0015667976021669177\n",
      "26. borsch 0.0015049762336636426\n",
      "27. rem_shkola 0.0014830383488445306\n",
      "28. bratishkinoff 0.0013244579432361977\n",
      "29. xfilm 0.001314705471976374\n",
      "30. ebashproklatie 0.0012310490746497002\n",
      "31. flatme 0.0012199914037346474\n",
      "32. ohhluul 0.0011456197890780007\n",
      "33. fuck_humor 0.00112713559125364\n",
      "34. girlmeme 0.001054652676853629\n",
      "35. zerofat 0.0010509477366451617\n",
      "36. leprazo 0.0010065220687759317\n",
      "37. sketch.books 0.0009622524333929813\n",
      "38. bot_maxim 0.0009400077828107293\n",
      "39. dzenpub 0.0009055085183891865\n",
      "40. thesmolny 0.0007869519682720428\n",
      "41. emptiness 0.0007638324419197206\n",
      "42. bestad 0.0006692296739285212\n",
      "43. privetuyeba 0.0006292209786237161\n",
      "44. s_arcazm 0.0005313691319100385\n",
      "45. mayyoung 0.0005221563646627181\n",
      "46. onlyorly 0.0005125601801581133\n",
      "47. otbrosy_pokoleniya 0.0005095191714248659\n",
      "48. club128730587 0.0004947695787390441\n",
      "49. pho 0.0004706859184749705\n",
      "50. trahninormalnost1 0.0004699942885937447\n",
      "51. staaywithmee 0.000454868514570006\n",
      "52. bon 0.00045073052575304027\n",
      "53. ultrapir 0.00042771599657827065\n",
      "54. best_video_hd 0.0004061808176441269\n",
      "55. medieval_or 0.00039301264426715073\n",
      "56. morgenshtern666 0.0003774031695262063\n",
      "57. pikabu 0.0003736737678555905\n",
      "58. 40kg 0.00037161252201072615\n",
      "59. overhear 0.0003267070782675185\n",
      "60. psy.people 0.00032354395561942824\n",
      "61. sci 0.00031461519148030527\n",
      "62. kinomania 0.0003066969913013473\n",
      "63. hmideas 0.0002948609941027784\n",
      "64. chop.choppp 0.000259112600214606\n",
      "65. tnt 0.00024515217092371626\n",
      "66. komment.broo 0.00022772620973658583\n",
      "67. nenormalnoo 0.0002124154201545251\n",
      "68. lixie 0.00017136813274152285\n",
      "69. i.kino 0.00017107712078053838\n",
      "70. mash 0.0001572765245622776\n",
      "71. soverwenstvo.decora 0.00015726685677902214\n",
      "72. mtn_news 0.00014364477497402772\n",
      "73. club52205838 0.00014214641080312712\n",
      "74. evil_incorparate 0.00014156745215072227\n",
      "75. iphone 0.00014149791517391967\n",
      "76. vandroukiru 0.00013084466119654228\n",
      "77. workart 0.0001159420289855088\n",
      "78. vkgames 0.00011533000917748707\n",
      "79. top_screens 0.00011396011396011355\n",
      "80. decadence 9.895991331831204e-05\n",
      "81. i_want_love_dream 8.539178927296683e-05\n",
      "82. romantika_for_you 7.128599942971178e-05\n",
      "83. psyxov 7.08775346382177e-05\n",
      "84. v5inf 7.074136955291429e-05\n",
      "85. igm 5.763071602524021e-05\n",
      "86. science_technology 5.685856432124936e-05\n",
      "87. kino_kaif 4.3478260869567185e-05\n",
      "88. exclusive_muzic 4.344048653344886e-05\n",
      "89. ru.esquire 2.8587764436819688e-05\n",
      "90. business 2.8477858465044323e-05\n",
      "91. team 2.838070585620874e-05\n",
      "92. officialpages 2.8153153153154255e-05\n",
      "93. beauty 1.4511681903935555e-05\n",
      "94. 21jqofa 1.4343086632243966e-05\n",
      "95. zloyshkolnik 1.4257199885943835e-05\n",
      "96. iloveu01 1.4257199885943835e-05\n",
      "97. lhack 1.4208581983517673e-05\n",
      "98. 9o_6o_9o 1.407596051400765e-05\n",
      "99. live 1.3957182373214306e-05\n",
      "100. vinevinevine 1.3829214223196976e-05\n",
      "101. i_des 2.4980207731969034e-07\n",
      "102. peregovorov 1.0651889363148352e-07\n",
      "103. fucking_humor 0.0\n",
      "104. lomai_loogiky 0.0\n",
      "105. pn6 0.0\n",
      "106. authors 0.0\n",
      "107. vkucnie_recepti 0.0\n",
      "108. vpovare 0.0\n",
      "109. great.food 0.0\n",
      "110. businessquotes 0.0\n",
      "111. ideasdecor 0.0\n",
      "112. op_manicure 0.0\n",
      "113. h.made 0.0\n",
      "114. v5umm 0.0\n",
      "115. vpisssska 0.0\n",
      "116. cook_good 0.0\n",
      "117. ifun 0.0\n",
      "118. vulgaarr 0.0\n",
      "119. ftp_memes 0.0\n",
      "120. homeideaz 0.0\n",
      "121. kino_mania 0.0\n",
      "122. r_a_y_s 0.0\n",
      "123. woman.blog 0.0\n",
      "124. be.beauty 0.0\n",
      "125. vk.krasota 0.0\n",
      "126. theastro 0.0\n",
      "127. be.women 0.0\n",
      "128. kinohd 0.0\n",
      "129. potracheno 0.0\n",
      "130. oldlentach 0.0\n",
      "131. gm.power 0.0\n",
      "132. recipes40kg 0.0\n",
      "133. e_goist 0.0\n",
      "134. molodost_bz 0.0\n",
      "135. 1poetry 0.0\n",
      "136. dfilm 0.0\n",
      "137. public_of_music 0.0\n",
      "138. combovine 0.0\n",
      "139. sh.cook 0.0\n",
      "140. modnailru -2.1704877172975164e-07\n",
      "141. life -2.960433478249591e-07\n",
      "142. academyofman -1.4124273445933106e-05\n",
      "143. videosos -1.4220517553472275e-05\n",
      "144. fortnite -1.4505366985784197e-05\n",
      "145. matchtv -2.847785846504062e-05\n",
      "146. face -2.851439977188397e-05\n",
      "147. marvel_dc -4.2716787697564634e-05\n",
      "148. vkmusic -4.27715996578278e-05\n",
      "149. femalemem -5.6973987583508466e-05\n"
     ]
    }
   ],
   "source": [
    "k = X.shape[1]\n",
    "names, imp, ind = most_important_features(importance, features, k)\n",
    "for i in range(k):\n",
    "    print(str(i+1) + \".\", names[i], imp[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем эксперимент! Давайте возьмем 80 фичей с наибольшей значимостью, а все остальные удалим. После 80 значимость почти нулевая у фичей, а у первых 80 значимость сильно не отличается, так что я не могу выделить какие-то определенные фичи, у которых значимость была бы сильно больше других. Так как другие фичи менее значимы, то предположим, что accuracy станет больше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
    "X_train1, X_test1, y_age_train1, y_age_test1, y_sex_train1, y_sex_test1 = train_test_split(X[:,ind[:80]], y_age, y_sex, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6176656151419558\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(criterion='gini', max_depth=6, min_samples_leaf=30,\n",
    "                                                       max_features = \"auto\", n_estimators=50)\n",
    "rfc.fit(X_train1, y_age_train1)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test1) == y_age_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
    "X_train, X_t, y_age_train, y_age_t, y_sex_train, y_sex_t = train_test_split(X, y_age, y_sex, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7829652996845425\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(criterion='gini', max_depth=6, min_samples_leaf=30,\n",
    "                                                       max_features = \"auto\", n_estimators=50)\n",
    "rfc.fit(X_train, y_sex_train)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_t) == y_sex_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сильно лучше не стало. Возможно это вообще случайность. Так что не понятно, имело ли это большой смысл. Если взять первые 10, то лучше точно не становится. Так что есть смысл убирать только те фичи, где значимость равна 0. У всех остальных все-таки значимость примерно одинаковая и нельзя выделить какие-то фичи, у которых значимость сильно больше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = np.zeros((100000, 6))\n",
    "# i = 0\n",
    "# #1 - gini, 2 - entropy\n",
    "# for crit in [1, 2]:\n",
    "#     for d in [4, 5, 6]:\n",
    "#         for msl in [5, 10, 30]:\n",
    "#             for mf in [3, 6, 12]:\n",
    "#                 for n_est in [10]:\n",
    "#                     rfc = RandomForestClassifier(criterion=crit, max_depth=d, min_samples_leaf=msl,\n",
    "#                                                        max_features=mf, n_estimators=n_est)\n",
    "#                     rfc.fit(X_train, y_sex_train)\n",
    "#                     acc =  np.mean(rfc.predict(X_val) == y_sex_val)\n",
    "#                     res[i,:] =[crit, d , msl, mf, n_est , acc]\n",
    "#                     i+=1\n",
    "#                     print(f'criterion = {crit}, ',f'depth = {d},', f'min_samples_leaf = {msl},', f'max_features = {mf}, ',\n",
    "#                          f'n_estimators = {n_est}')\n",
    "#                     print(\"Accuracy:\", np.mean(rfc.predict(X_val) == y_sex_val))\n",
    "\n",
    "# print(res[np.argmax(res[:,-1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort_res = res[np.argsort(res[:,-1])][::-1]\n",
    "# sort_res[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8234552332912989\n",
      "Most important features:\n",
      "1. 40kg 0.015175897291217887\n",
      "2. mudakoff 0.014492404988749501\n",
      "3. girlmeme 0.008919733291669264\n",
      "4. zerofat 0.008896433570138324\n",
      "5. 9o_6o_9o 0.008485462289480598\n",
      "6. rapnewrap 0.008342413972604934\n",
      "7. igm 0.00682615388348498\n",
      "8. cook_good 0.005384617115125876\n",
      "9. be.beauty 0.004251458593657704\n",
      "10. bon 0.0040182957713958325\n",
      "11. modnailru 0.00384995237427342\n",
      "12. i_d_t 0.0035971063086401166\n",
      "13. woman.blog 0.0035874210967665344\n",
      "14. be.women 0.003568631166180023\n",
      "15. beauty 0.003518277064544324\n",
      "16. femalemem 0.003367311815130756\n",
      "17. sh.cook 0.0032681873730482634\n",
      "18. thesmolny 0.0031666768845773174\n",
      "19. reflexia_our_feelings 0.002968639869080234\n",
      "20. fuck_humor 0.00291928022332036\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(criterion='gini', max_depth=6, min_samples_leaf=10,\n",
    "                             max_features=\"auto\", n_estimators=50)\n",
    "rfc.fit(X_train, y_sex_train)\n",
    "print(\"Accuracy:\", np.mean(rfc.predict(X_test) == y_sex_test))\n",
    "print(\"Most important features:\")\n",
    "importance = feature_importance(rfc)\n",
    "k = 20\n",
    "names, imp, ind = most_important_features(importance, features, k)\n",
    "for i in range(k):\n",
    "    print(str(i+1) + \".\", names[i], imp[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost\n",
    "В качестве альтернативы попробуем CatBoost. \n",
    "\n",
    "Установить его можно просто с помощью `pip install catboost`. Туториалы можно найти, например, [здесь](https://catboost.ai/docs/concepts/python-usages-examples.html#multiclassification) и [здесь](https://github.com/catboost/tutorials/blob/master/python_tutorial.ipynb). Главное - не забудьте использовать `loss_function='MultiClass'`.\\\n",
    "\\\n",
    "Сначала протестируйте CatBoost на синтетических данных. Выведите точность и важность признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.0074859\ttotal: 83.8ms\tremaining: 335ms\n",
      "1:\tlearn: 0.9254973\ttotal: 86.9ms\tremaining: 130ms\n",
      "2:\tlearn: 0.8557739\ttotal: 89.9ms\tremaining: 60ms\n",
      "3:\tlearn: 0.7937512\ttotal: 92.9ms\tremaining: 23.2ms\n",
      "4:\tlearn: 0.7364863\ttotal: 95.7ms\tremaining: 0us\n",
      "Accuracy: 1.0\n",
      "Importance: [5.03193201e-04 8.11686844e-04 2.81763728e+01 2.81780796e+01\n",
      " 4.36441847e+01 4.79515220e-05]\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "X, y = synthetic_dataset(1000)\n",
    "\n",
    "model = CatBoostClassifier(iterations=5,\n",
    "                           learning_rate=0.05,\n",
    "                           depth=5,\n",
    "                           loss_function='MultiClass')\n",
    "model.fit(X, y, verbose=True)\n",
    "print(\"Accuracy:\", np.mean(model.predict(X) ==  y.reshape(1000,1)))\n",
    "print(\"Importance:\", model.get_feature_importance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5 (3 балла)\n",
    "Попробуем применить один из используемых на практике алгоритмов. В этом нам поможет CatBoost. Также, как и реализованный ними RandomForest, применим его для определения пола и возраста пользователей сети Вконтакте, выведите названия наиболее важных признаков так же, как в задании 3.\\\n",
    "\\\n",
    "Эксперименты с множеством используемых признаков и подбор гиперпараметров приветствуются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y_age, y_sex, features = read_dataset(\"vk.csv\")\n",
    "X_train, X_test, y_age_train, y_age_test, y_sex_train, y_sex_test = train_test_split(X, y_age, y_sex, train_size=0.9)\n",
    "X_train, X_eval, y_age_train, y_age_eval, y_sex_train, y_sex_eval = train_test_split(X_train, y_age_train, y_sex_train, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возраст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7351828499369483\n",
      "Most important features:\n",
      "1. ovsyanochan\n",
      "2. styd.pozor\n",
      "3. 4ch\n",
      "4. rhymes\n",
      "5. mudakoff\n",
      "6. leprum\n",
      "7. bestad\n",
      "8. xfilm\n",
      "9. bot_maxim\n",
      "10. rapnewrap\n",
      "11. iwantyou\n",
      "12. dayvinchik\n",
      "13. pravdashowtop\n",
      "14. tumblr_vacuum\n",
      "15. fuck_humor\n",
      "16. dzenpub\n",
      "17. thesmolny\n",
      "18. lixie\n",
      "19. i_d_t\n",
      "20. borsch\n"
     ]
    }
   ],
   "source": [
    "model_age = CatBoostClassifier(iterations=1000,\n",
    "                           learning_rate=0.1,\n",
    "                           depth=4,\n",
    "                           random_seed=42,\n",
    "                           loss_function='MultiClass')\n",
    "model_age.fit(X_train,y_age_train, verbose=False, eval_set=(X_eval, y_age_eval))\n",
    "\n",
    "print(\"Accuracy:\", np.mean(model_age.predict(X_test) == y_age_test.reshape(y_age_test.shape[0],1)))\n",
    "importance = model_age.get_feature_importance()\n",
    "\n",
    "print(\"Most important features:\")\n",
    "k = 20\n",
    "names, imp,ind = most_important_features(importance, features, k)\n",
    "for i in range(k):\n",
    "    print(str(i+1) + \".\", names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пол"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8839848675914249\n",
      "Most important features:\n",
      "1. 40kg\n",
      "2. modnailru\n",
      "3. girlmeme\n",
      "4. i_d_t\n",
      "5. mudakoff\n",
      "6. 9o_6o_9o\n",
      "7. zerofat\n",
      "8. femalemem\n",
      "9. igm\n",
      "10. academyofman\n",
      "11. be.beauty\n",
      "12. sh.cook\n",
      "13. thesmolny\n",
      "14. reflexia_our_feelings\n",
      "15. be.women\n",
      "16. woman.blog\n",
      "17. h.made\n",
      "18. recipes40kg\n",
      "19. rapnewrap\n",
      "20. team\n"
     ]
    }
   ],
   "source": [
    "model_sex = CatBoostClassifier(iterations=1000,\n",
    "                           learning_rate=0.1,\n",
    "                           depth=4,\n",
    "                           random_seed=42,\n",
    "                           loss_function='MultiClass')\n",
    "model_sex.fit(X_train,y_sex_train, verbose=False, eval_set=(X_eval, y_sex_eval))\n",
    "\n",
    "print(\"Accuracy:\", np.mean(model_sex.predict(X_test) == y_sex_test.reshape(y_sex_test.shape[0],1)))\n",
    "importance = model_sex.get_feature_importance()\n",
    "\n",
    "print(\"Most important features:\")\n",
    "k = 20\n",
    "names, imp, ind = most_important_features(importance, features, k)\n",
    "for i in range(k):\n",
    "    print(str(i+1) + \".\", names[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
